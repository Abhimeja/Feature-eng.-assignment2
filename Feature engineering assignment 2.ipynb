{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e0f3788f-2b42-4a47-b591-4dfa2b9b2b10",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Ans--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abd187-d4bb-49fa-8de9-a2e4955e677a",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features in a dataset to a common scale, typically between 0 and 1. This scaling method is often applied to prevent certain features from dominating others when the features have different ranges. It's especially useful for algorithms that are sensitive to the scale of input data, such as gradient descent-based optimization algorithms.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c5144-46de-4916-95f6-fb5f4c151b50",
   "metadata": {},
   "source": [
    "X_scaled = (X - X_min) / (X_max - X_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ba0f3-7200-4555-9e0c-126fcfe584f7",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "X_scaled is the scaled value of the feature.\n",
    "\n",
    "X is the original value of the feature.\n",
    "\n",
    "X_min is the minimum value of the feature in the dataset.\n",
    "\n",
    "X_max is the maximum value of the feature in the dataset.\n",
    "\n",
    "Here's an example in Python to illustrate how Min-Max scaling works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289411f1-a8d7-49a0-ad45-4f480eaa3dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[ 2.  5.]\n",
      " [10. 20.]\n",
      " [ 1.  8.]]\n",
      "\n",
      "Scaled Data:\n",
      "[[0.11111111 0.        ]\n",
      " [1.         1.        ]\n",
      " [0.         0.2       ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Sample data with different range\n",
    "data=np.array([[2.0,5.0],\n",
    "              [10.0,20.0],\n",
    "              [1.0,8.0]])\n",
    "\n",
    "#Initialize the MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "\n",
    "#Fit and Transform the data using the scaler\n",
    "scaled_data=scaler.fit_transform(data)\n",
    "\n",
    "print('Original Data:')\n",
    "print(data)\n",
    "print('\\nScaled Data:')\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536edca-2432-4d8d-913d-550bb93ac24c",
   "metadata": {},
   "source": [
    "In this example, the original data has different ranges for each feature. After applying Min-Max scaling, the values are transformed to the [0, 1] range. The first column values are scaled down, and the second column values are scaled up to ensure that both features have the same scale. This can be particularly useful when using machine learning algorithms that rely on distance metrics or gradient-based optimization."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4929e75b-8cdb-4c39-a081-80e2a4d2b516",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "Ans--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3ebaf-f09a-4405-bb4c-6ddb6aee36b6",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Vector Normalization or L2 Normalization, is a feature scaling method used to scale the feature vectors to have a Euclidean norm (magnitude) of 1. This technique ensures that each feature vector is transformed into a unit vector, maintaining the direction of the original vector while adjusting its length.\n",
    "\n",
    "The formula for Unit Vector scaling is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f684a4-a2f3-4f39-af39-1575df22035b",
   "metadata": {},
   "source": [
    "X_normalized = X / ||X||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522d345-5913-4125-ad98-ac6cac408929",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "X_normalized is the normalized vector.\n",
    "\n",
    "X is the original vector.\n",
    "\n",
    "||X|| represents the Euclidean norm (magnitude) of the vector X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb56c7e-761f-4d3f-af1f-f3fac0fe70d5",
   "metadata": {},
   "source": [
    "Compared to Min-Max scaling, which scales features within a specific range (usually [0, 1]), Unit Vector scaling focuses on maintaining the relative directions of the feature vectors and not on adjusting their range.\n",
    "\n",
    "Here's an example in Python to illustrate the Unit Vector scaling technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cef2fa3-07a7-4f51-9e31-ea3417421136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[ 2.  5.]\n",
      " [10. 20.]\n",
      " [ 1.  8.]]\n",
      "\n",
      "Normalized Data:\n",
      "[[0.37139068 0.92847669]\n",
      " [0.4472136  0.89442719]\n",
      " [0.12403473 0.99227788]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Sample data with different ranges\n",
    "data = np.array([[2.0, 5.0],\n",
    "                 [10.0, 20.0],\n",
    "                 [1.0, 8.0]])\n",
    "\n",
    "# Normalize the data using L2 normalization\n",
    "normalized_data = normalize(data, norm='l2')\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e470d9a-4eb2-4ede-8b1d-4cf545132d01",
   "metadata": {},
   "source": [
    "In this example, the original data's feature vectors are normalized using the L2 normalization technique. Notice that the direction of each vector is preserved, but their lengths (Euclidean norms) are scaled down to 1. The resulting vectors are unit vectors with respect to their directions. This technique can be helpful when you're interested in the relative relationships between feature vectors rather than their absolute magnitudes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fbfec1b-2335-4937-ad45-45af8dc00cad",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "Ans--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430e89a-df12-4ff4-935d-4c8204b2008f",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a widely used technique in machine learning and data analysis for reducing the dimensionality of high-dimensional data while retaining as much of the original variance as possible. It achieves this by transforming the original features into a new set of uncorrelated features called principal components. These components are linear combinations of the original features, and they are sorted by the amount of variance they capture. The first principal component captures the most variance, followed by the second, and so on.\n",
    "\n",
    "PCA works by finding the eigenvectors and eigenvalues of the covariance matrix of the data and then projecting the data onto the new space defined by these eigenvectors.\n",
    "\n",
    "Here's an example in Python to illustrate how PCA is used for dimensionality reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f248afc-7ea2-4995-86bb-14ea8c225dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[2.5 2.4]\n",
      " [0.5 0.7]\n",
      " [2.2 2.9]\n",
      " [1.9 2.2]\n",
      " [3.1 3. ]\n",
      " [2.3 2.7]\n",
      " [2.  1.6]\n",
      " [1.  1.1]\n",
      " [1.5 1.6]\n",
      " [1.1 0.9]]\n",
      "\n",
      "Reduced Data:\n",
      "[[-0.82797019]\n",
      " [ 1.77758033]\n",
      " [-0.99219749]\n",
      " [-0.27421042]\n",
      " [-1.67580142]\n",
      " [-0.9129491 ]\n",
      " [ 0.09910944]\n",
      " [ 1.14457216]\n",
      " [ 0.43804614]\n",
      " [ 1.22382056]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data with high dimensionality\n",
    "data = np.array([[2.5,2.4],\n",
    "                [0.5,0.7],\n",
    "                [2.2, 2.9],\n",
    "                [1.9, 2.2],\n",
    "                [3.1, 3.0],\n",
    "                [2.3, 2.7],\n",
    "                [2.0, 1.6],\n",
    "                [1.0, 1.1],\n",
    "                [1.5, 1.6],\n",
    "                [1.1, 0.9]])\n",
    "\n",
    "# Initialize the PCA model with 1 principal component\n",
    "pca=PCA(n_components=1)\n",
    "\n",
    "# Fit and transform the data using PCA\n",
    "data_reduced=pca.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nReduced Data:\")\n",
    "print(data_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d98296-94c2-490a-a69e-424e0aa7f21b",
   "metadata": {},
   "source": [
    "In this example, we start with a dataset of two-dimensional data points. We use PCA to reduce the dimensionality to one principal component. The data is projected onto the direction of the first principal component, which captures the most significant variance in the data. As a result, the reduced data has only one dimension while preserving as much of the original variance as possible."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8699125-bbef-4f4d-a15b-094950859c7a",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Ans--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf65075-8043-4f99-9e45-33caa85aa8da",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts in the field of machine learning and data analysis. Feature extraction is the process of transforming the original features of a dataset into a new set of features that captures the most relevant information while reducing dimensionality. PCA is a specific technique that can be used for feature extraction.\n",
    "\n",
    "PCA can be used for feature extraction by transforming the original features into a new set of uncorrelated features called principal components. These principal components are linear combinations of the original features and are chosen in a way that they capture the maximum variance in the data. By selecting a subset of the principal components, you can effectively reduce the dimensionality of the data while retaining the most important information.\n",
    "\n",
    "Here's an example in Python to illustrate how PCA can be used for feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211f3b3c-c0be-4b93-97ed-122fc82af616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[2.5 2.4 0.5]\n",
      " [0.5 0.7 1.2]\n",
      " [2.2 2.9 1.5]\n",
      " [1.9 2.2 3.6]\n",
      " [3.1 3.  0.8]\n",
      " [2.3 2.7 2.8]\n",
      " [2.  1.6 2.2]\n",
      " [1.  1.1 0.1]\n",
      " [1.5 1.6 2. ]\n",
      " [1.1 0.9 1.5]]\n",
      "\n",
      "Extracted Features:\n",
      "[[-0.05650048  1.39808861]\n",
      " [ 1.70510165 -0.65305271]\n",
      " [-0.76384384  0.63678761]\n",
      " [-1.33681867 -1.49144507]\n",
      " [-0.92621545  1.62552789]\n",
      " [-1.42014559 -0.47406394]\n",
      " [-0.23432723 -0.51960491]\n",
      " [ 1.79730459  0.61586269]\n",
      " [ 0.15033361 -0.56033949]\n",
      " [ 1.08511142 -0.57776067]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample dataset with high dimensionality\n",
    "data = np.array([[2.5, 2.4, 0.5],\n",
    "                 [0.5, 0.7, 1.2],\n",
    "                 [2.2, 2.9, 1.5],\n",
    "                 [1.9, 2.2, 3.6],\n",
    "                 [3.1, 3.0, 0.8],\n",
    "                 [2.3, 2.7, 2.8],\n",
    "                 [2.0, 1.6, 2.2],\n",
    "                 [1.0, 1.1, 0.1],\n",
    "                 [1.5, 1.6, 2.0],\n",
    "                 [1.1, 0.9, 1.5]])\n",
    "\n",
    "# Initialize the PCA model\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data using PCA for feature extraction\n",
    "data_extracted = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nExtracted Features:\")\n",
    "print(data_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae16d3e-3b4a-4b4b-9947-af3cbec14e0c",
   "metadata": {},
   "source": [
    "In this example, we apply PCA for feature extraction on a dataset with three original features. By setting n_components=2, we are extracting two principal components. The extracted features are a lower-dimensional representation of the original data that retains the most important information while reducing the dimensionality from 3 to 2. These extracted features can then be used for further analysis or as input to machine learning algorithms."
   ]
  },
  {
   "cell_type": "raw",
   "id": "24ba637f-560c-4396-9d00-bda73c30dc16",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "Ans--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f1dc6-fd2f-43dd-afdd-6cdfff0dd72f",
   "metadata": {},
   "source": [
    "Building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the features such as price, rating, and delivery time before using them in the recommendation algorithm. Min-Max scaling will ensure that these features are on a common scale, which is important for some recommendation algorithms that use distance-based metrics or optimization methods.\n",
    "\n",
    "Here's how you would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "1.Understand the Features: First, you need to understand the range and distribution of each feature in your dataset. For example, price might range from low to high values, rating might be on a scale of 1 to 5, and delivery time might vary from a few minutes to hours.\n",
    "\n",
    "2.Apply Min-Max Scaling: For each feature, apply the Min-Max scaling formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206ac3a-a9bc-46de-8f01-d4a4b812b3ea",
   "metadata": {},
   "source": [
    "X_scaled = (X - X_min) / (X_max - X_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b354fd-32b9-498e-aa1e-0cb53b6279f0",
   "metadata": {},
   "source": [
    "Where X is the original value of the feature, X_min is the minimum value of the feature in the dataset, and X_max is the maximum value of the feature in the dataset.\n",
    "\n",
    "3.Transform the Features: Transform each feature using the Min-Max scaling formula. This will map the original values to a common scale between 0 and 1.\n",
    "\n",
    "4.Use Scaled Features in Recommendation System: Once the features are scaled, you can use them as input to your recommendation system. Algorithms like collaborative filtering or content-based filtering will benefit from having features on the same scale, as it prevents one feature from dominating the recommendations due to its larger magnitude.\n",
    "\n",
    "Here's a Python code snippet that demonstrates how to apply Min-Max scaling using the MinMaxScaler from the sklearn.preprocessing module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d42aee8c-df3a-4b45-bfeb-8c1e31dff6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[10.   4.5 30. ]\n",
      " [25.   3.8 45. ]\n",
      " [15.   4.2 20. ]\n",
      " [30.   4.9 15. ]]\n",
      "\n",
      "Scaled Data:\n",
      "[[0.         0.63636364 0.5       ]\n",
      " [0.75       0.         1.        ]\n",
      " [0.25       0.36363636 0.16666667]\n",
      " [1.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data for price, rating, and delivery time\n",
    "data = np.array([[10.0, 4.5, 30],\n",
    "                 [25.0, 3.8, 45],\n",
    "                 [15.0, 4.2, 20],\n",
    "                 [30.0, 4.9, 15]])\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497d778-5178-49ce-82ab-17be99325dcc",
   "metadata": {},
   "source": [
    "In this example, the scaled_data will contain the scaled features. Each column (feature) will have values between 0 and 1, which can be directly used in your recommendation system to ensure that the features are on a common scale and no particular feature dominates the recommendation process."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e15a98a2-4ea3-40b4-9986-73ea671ed952",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "Ans--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcc51e-2f9d-432a-b29b-c7e7a33f3173",
   "metadata": {},
   "source": [
    "When dealing with a dataset containing many features, such as in the case of predicting stock prices with features like company financial data and market trends, dimensionality reduction techniques like Principal Component Analysis (PCA) can be employed to simplify the dataset and potentially improve the performance of your predictive model. Here's how you can use PCA for this purpose:\n",
    "\n",
    "1.Data Preprocessing: Before applying PCA, it's important to preprocess your data. This involves handling missing values, scaling the features (using techniques like Min-Max scaling or standardization), and ensuring that your features are on a comparable scale.\n",
    "\n",
    "2.Calculate Covariance Matrix: PCA is based on the covariance matrix of your data. Calculate the covariance matrix of your feature matrix. This matrix represents the relationships and variances among your original features.\n",
    "\n",
    "3.Compute Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance in the data, and eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "4.Select Principal Components: Sort the eigenvalues in descending order and choose the top k eigenvectors corresponding to the highest eigenvalues. These eigenvectors are the principal components that capture the most significant variance in the data.\n",
    "\n",
    "5.Project Data onto Principal Components: Project your original data onto the selected principal components. This involves calculating the dot product between your data matrix and the matrix of selected eigenvectors.\n",
    "\n",
    "6.Dimensionality Reduction: The projected data will have reduced dimensions, with each instance represented by the values along the chosen principal components. You can choose to keep as many principal components as necessary to capture a certain percentage of the total variance (e.g., 95%).\n",
    "\n",
    "7.Use Reduced-Dimension Data: The reduced-dimension data can be used as input for your predictive model. Since you've retained the most important variance in the data, your model can potentially perform well with fewer features, which could help with generalization and reducing overfitting.\n",
    "\n",
    "Here's a high-level example of how you might use PCA in Python for your stock price prediction project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "932552dc-bceb-48e3-ad93-722dae71a73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratios: [0.10042093 0.07954537 0.07506969 0.07426424 0.06321741 0.06194698\n",
      " 0.05883652 0.05736416 0.05371269 0.0489051 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset with many features (rows are samples, columns are features)\n",
    "\n",
    "data=np.random.rand(100,20) # Example dataset with 100 samples and 20 features\n",
    "\n",
    "#Standarize the data\n",
    "scaler=StandardScaler()\n",
    "data_standardize=scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA with a desired number of components\n",
    "n_components=10 # Example: reduce to 10 principal components\n",
    "pca=PCA(n_components=n_components)\n",
    "data_reduced=pca.fit_transform(data_standardize)\n",
    "\n",
    "# Check the explained variance ratio for each component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratios:\", explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59628f4c-33b3-4403-ae15-92451c35c180",
   "metadata": {},
   "source": [
    "In this example, the data_reduced will have fewer features (dimensions) than the original data, while still capturing a significant portion of the original variance. These reduced features can then be used as inputs to build and train your stock price prediction model. Keep in mind that the exact number of components to retain and the impact on prediction accuracy will depend on the characteristics of your dataset and the specific predictive modeling techniques you're using."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ceabf21-e811-4530-aec8-8c49041d8acb",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "\n",
    "Ans--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d6fac-ecc9-4bc7-9262-5dd89d4ad830",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on the given dataset to transform the values to a range of -1 to 1, you need to follow these steps:\n",
    "\n",
    "Calculate the minimum and maximum values in the dataset.\n",
    "\n",
    "1.Apply the Min-Max scaling formula to each value in the dataset.\n",
    "\n",
    "2.The Min-Max scaling formula for transforming values from the original range to a new range [a, b] is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb27068-8381-4bde-83f9-b533c96eb275",
   "metadata": {},
   "source": [
    "X_scaled = a + (X - X_min) * (b - a) / (X_max - X_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dac57c-7270-43fb-8da1-5558d1534508",
   "metadata": {},
   "source": [
    "In this case, you want to transform the values to a range of -1 to 1, so a = -1 and b = 1.\n",
    "\n",
    "Here's the calculation for the given dataset: [1, 5, 10, 15, 20]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c82b06-bf38-4973-b92d-76512a91ea87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 1  5 10 15 20]\n",
      "Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Given Dataset\n",
    "data=np.array([1,5,10,15,20])\n",
    "\n",
    "#Define range\n",
    "a=-1\n",
    "b=1\n",
    "\n",
    "# Calculate the minimum and maximum values\n",
    "X_min=np.min(data)\n",
    "X_max=np.max(data)\n",
    "\n",
    "#Apply Min-Max Scaling\n",
    "scaled_data=a + (data - X_min) * (b-a)/(X_max - X_min)\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Scaled Data:\", scaled_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6d7cda2-2e33-4895-988e-4c5ca917a97a",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "Ans--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b3091-d0b7-40d9-bcce-ec42cf9259c3",
   "metadata": {},
   "source": [
    "When performing feature extraction using PCA, one of the key decisions you need to make is how many principal components to retain. This decision involves balancing the trade-off between reducing dimensionality and preserving the variance in the data. The number of principal components you choose to retain depends on the specific characteristics of your dataset, your goals, and the amount of variance you're willing to preserve.\n",
    "\n",
    "Here's a general approach to deciding how many principal components to retain:\n",
    "\n",
    "1.Calculate Explained Variance Ratio: After applying PCA to your dataset, you'll obtain a set of principal components along with their corresponding explained variance ratios. The explained variance ratio of a principal component indicates the proportion of the total variance in the data that it captures. The cumulative sum of these ratios tells you how much variance is captured by the first k principal components.\n",
    "\n",
    "2.Choose a Threshold: Decide on a threshold for the amount of variance you want to retain. For example, you might aim to retain 95% or 99% of the total variance. This threshold should reflect how much information you're willing to preserve in the reduced-dimensional representation of your data.\n",
    "\n",
    "3.Find the Appropriate Number of Components: Identify the smallest number of principal components (k) that, when summed, exceed or meet your chosen variance threshold. This means that the cumulative explained variance ratio of the first k principal components should be close to or greater than your chosen threshold.\n",
    "\n",
    "4.Reasoning for the Chosen Number: Consider the number of principal components you've chosen in the context of your analysis. Does the retained number of components capture the essential patterns and relationships in your data? Does it align with the goals of your analysis or predictive modeling task?\n",
    "\n",
    "It's worth noting that there's no one-size-fits-all answer to how many principal components to retain. The choice depends on factors such as the complexity of the data, the amount of variance you're willing to lose, and the computational resources available. Retaining too few principal components might result in loss of important information, while retaining too many might lead to overfitting or increased computational costs.\n",
    "\n",
    "Here's a rough example of how the process might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b80be2-4e31-49b1-83e1-b39b96c2e505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
